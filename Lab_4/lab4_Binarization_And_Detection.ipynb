{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ac7b05",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4. Цифровая обработка изображений.\n",
    "\n",
    "В этой работе рассматриваются различные виды цифровой обработки изображений.\n",
    "\n",
    "Цель лабораторной работы:\n",
    "1. Бинаризация\n",
    "2. Выделение границ\n",
    "3. Методы анализа изображений\n",
    "4. Локальные особенности. Особые точки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e295b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63435890",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5f6e97cb0f2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutility\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from utility import util\n",
    "\n",
    "# Изменим стандартный размер графиков matplotlib\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ba625",
   "metadata": {},
   "source": [
    "## 1 Бинаризация\n",
    "\n",
    "В обработке изображений часто используется процедура пороговой бинаризации -- разбиения изображения на две области, одна из которых содержит все пиксели со значением ниже некоторого порога, а другая содержи все пиксели со значением выше этого порога.\n",
    "Оптимальная пороговая сегментация основана на приближении гистограммы изображения к некоторой кривой с использованием весовых сумм двух или более вероятностей интенсивности с нормальным распределением. Тогда порог - это набор ближайших уровней яркости, соответствующих минимуму вероятности между максимумами двух или более нормальных распределений.\n",
    "\n",
    "\n",
    "<img src=\"../content/binarization_examle.png\" width=\"800\"/>\n",
    "\n",
    "Примеры бинаризации изображений будем рассматривать на изображении хлорелл под микроскопом\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv.imread('../images/bloodcells.png')\n",
    "image2 = cv.imread('../images/chlorella.png')\n",
    "rgb_image1 = cv.cvtColor(image1, cv.COLOR_BGR2RGB)\n",
    "hsv_image1 = cv.cvtColor(image1, cv.COLOR_BGR2HSV)\n",
    "gray_image1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY)\n",
    "rgb_image2 = cv.cvtColor(image2, cv.COLOR_BGR2RGB)\n",
    "hsv_image2 = cv.cvtColor(image2, cv.COLOR_BGR2HSV)\n",
    "gray_image2 = cv.cvtColor(image2, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "channels = [0]\n",
    "histSize = [256]\n",
    "range = [0, 256]\n",
    "\n",
    "gs = plt.GridSpec(2, 2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(gray_image1, cmap='gray')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(gray_image2, cmap='gray')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(gs[2])\n",
    "plt.hist(gray_image1.reshape(-1), 256, range)\n",
    "plt.subplot(gs[3])\n",
    "plt.hist(gray_image2.reshape(-1), 256, range)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524b58c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.1 Бинаризация полутоновых изображений (пороговая фильтрация).\n",
    "\n",
    "Рассмотрим простую бинаризацию на примере полутонового изображения.\n",
    "\n",
    "Бинаризация полутоновых изображений осуществляется просто. Для каждого пикселя применяется одно и то же пороговое значение. Если значение пикселя меньше порогового значения, оно имеет значение 0, в противном случае — максимальное значение. В библиотеке OpenCV есть функция для бинаризации изображений cv.threshold(), для применения пороговых значений. Функция принимает несколько параметров:\n",
    "- image -- изображение, к которому применяется бинаризация;\n",
    "- threshold -- пороговое значение;\n",
    "- maxval -- максимальное значение, которое присваивается значениям пикселей, превышающим пороговое значение;\n",
    "- type -- тип порога.\n",
    "\n",
    "OpenCV предоставляет различные типы пороговых значений:\n",
    "- cv.THRESH_BINARY\n",
    "$$\n",
    "\\begin{equation*}\n",
    "out(x, y) =\n",
    " \\begin{cases}\n",
    "   maxval \\; &\\textit{if image(x, y) > threshold } \\\\\n",
    "   0 \\; &\\textit{иначе}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "- cv.THRESH_BINARY_INV\n",
    "$$\n",
    "\\begin{equation*}\n",
    "out(x, y) =\n",
    " \\begin{cases}\n",
    "   0 \\; &\\textit{if image(x, y) > threshold } \\\\\n",
    "   maxval \\; &\\textit{иначе}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "- cv.THRESH_TRUNC\n",
    "$$\n",
    "\\begin{equation*}\n",
    "out(x, y) =\n",
    " \\begin{cases}\n",
    "   threshold \\; &\\textit{if image(x, y) > threshold } \\\\\n",
    "   image(x,y) \\; &\\textit{иначе}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "- cv.THRESH_TOZERO\n",
    "$$\n",
    "\\begin{equation*}\n",
    "out(x, y) =\n",
    " \\begin{cases}\n",
    "   image(x, y) \\; &\\textit{if image(x, y) > threshold } \\\\\n",
    "   0 \\; &\\textit{иначе}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "- cv.THRESH_TOZERO_INV\n",
    "$$\n",
    "\\begin{equation*}\n",
    "out(x, y) =\n",
    " \\begin{cases}\n",
    "   0 \\; &\\textit{if image(x, y) > threshold } \\\\\n",
    "   image(x, y) \\; &\\textit{иначе}\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Подробнее по типам смотри [документацию](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#gaa9e58d2860d4afa658ef70a9b1115576)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 150\n",
    "image = gray_image1\n",
    "\n",
    "ret, thresh1 = cv.threshold(image, threshold, 255, cv.THRESH_BINARY)\n",
    "ret, thresh2 = cv.threshold(image, threshold, 255, cv.THRESH_BINARY_INV)\n",
    "ret, thresh3 = cv.threshold(image, threshold, 255, cv.THRESH_TRUNC)\n",
    "ret, thresh4 = cv.threshold(image, threshold, 255, cv.THRESH_TOZERO)\n",
    "ret, thresh5 = cv.threshold(image, threshold, 255, cv.THRESH_TOZERO_INV)\n",
    "titles = ['Grayscale Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']\n",
    "images = [image, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in np.arange(len(images)):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(images[i], 'gray', vmin=0, vmax=255)\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55ff88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0272e49",
   "metadata": {},
   "source": [
    "### 1.2 Бинаризация Оцу (Otsu's Binarization)\n",
    "\n",
    "Для определения оптимального порога бинаризации предложено большое количество различных подходов. Наиболее удачным из них является подход Оцу, который предполагает не только определение оптимального порога бинаризации, но и вычисление некоторого критерия бимодальности, т.е оценку того, действительно ли исследуемая гистограмма содержит именно две моды (два выраженных пика).\n",
    "Подробнее про метод Оцу и алгоритм его работы можно почитать [здесь](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html) и [здесь](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%9E%D1%86%D1%83)\n",
    "\n",
    "Для использования этого метода в opencv используется таже функция cv.threshold(), в которой в качестве дополнительного флага передается параметр cv.THRESH_OTSU. Пороговое значение может быть выбрано произвольным. Затем алгоритм находит оптимальное пороговое значение, которое возвращается в качестве первого значения кортежа.\n",
    "\n",
    "Работу метода Оцу рассмотрим на примере:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f90099",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 150\n",
    "ret1, thresh1 = cv.threshold(image, threshold, 255, cv.THRESH_BINARY)\n",
    "ret2, thresh2 = cv.threshold(image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "titles = ['Original Image', 'Global Thresholding (threshold = %d)' % threshold,\n",
    "          \"Otsu's Thresholding (Otsu's threshold = %d)\" % ret2]\n",
    "images = [image, thresh1, thresh2]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in np.arange(len(images)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(images[i], 'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e067eb3",
   "metadata": {},
   "source": [
    "### 1.3 Адаптивная бинаризация.\n",
    "\n",
    "В простой полутоновой бинаризации в качестве порога используется одно значение. Но это может быть не во всех случаях, например, если изображение имеет разные условия освещения в разных областях. В этом случае может помочь адаптивное пороговое значение. Здесь алгоритм определяет порог для пикселя на основе небольшой области вокруг него. Таким образом, мы получаем разные пороги для разных областей одного и того же изображения, что дает лучшие результаты для изображений с различной освещенностью.\n",
    "\n",
    "Помимо описанных выше параметров, метод `cv.adaptiveThreshold()` принимает три входных параметра:\n",
    "\n",
    "- **AdaptiveMethod** решает, как вычисляется пороговое значение:\n",
    "    + `cv.ADAPTIVE_THRESH_MEAN_C`: Пороговое значение представляет собой среднее значение площади окрестностей минус константа C.\n",
    "    + `cv.ADAPTIVE_THRESH_GAUSSIAN_C`: Пороговое значение представляет собой гауссово-взвешенную сумму значений окрестностей минус константа C.\n",
    "- **BlockSize** определяет размер области соседства, а C — константа, которая вычитается из средней или взвешенной суммы соседних пикселей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851554fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret1, thresh1 = cv.threshold(image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "thresh2 = cv.adaptiveThreshold(image, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 15, 5)\n",
    "thresh3 = cv.adaptiveThreshold(image, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 15, 5)\n",
    "titles = ['Original Image', \"Otsu's Thresholding (Otsu's threshold = %d)\" % ret1,\n",
    "          'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [image, thresh1, thresh2, thresh3]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in np.arange(len(images)):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(images[i], 'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f46f03",
   "metadata": {},
   "source": [
    "### 1.4 Сегментация многомодальных изображений (Мультипороговая гистограммная бинаризация)\n",
    "\n",
    "<img src=\"../content/multimodal_segmentation.png\" width=\"800\"/>\n",
    "\n",
    "Сегментация многомодальных изображений - специально разработанный для данного класса задач, метод статистического выделения мод позволяет оценивать количество и степень выраженности мод гистограммы, опираясь на соответствующий график статистической производной (функции локальной разделимости), представляющий собой график значений критерия Оцу, вычисляемых в локальном скользящем окне, согласованном по ширине с ожидаемой шириной моды гистограммы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf97bc",
   "metadata": {},
   "source": [
    "### 1.5 Цветовая сегментация изображений\n",
    "\n",
    "Известно, что цветные цифровые изображения представляют собой совокупность трех цветовых плоскостей, каждая из которых характеризует одну независимую составляющую цвета, представленную в том же формате, что и обычное 8-битное полутоновое изображение. Следовательно, все описанные процедуры обработки полутоновых изображений в яркостной области могут быть обобщены и на случай обработки цветных изображений. Специфика же здесь связана прежде всего с различными цветовыми моделями, позволяющими по-разному работать с разными цветовыми и другими составляющими изображения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = cv.imread('../images/pencils.png')\n",
    "rgb_image3 = cv.cvtColor(image3, cv.COLOR_BGR2RGB)\n",
    "hsv_image3 = cv.cvtColor(rgb_image3, cv.COLOR_RGB2HSV)\n",
    "h, s, v = cv.split(hsv_image3)\n",
    "\n",
    "low_h = 75\n",
    "high_h = 85\n",
    "\n",
    "mask = cv.inRange(h, low_h, high_h)\n",
    "result = cv.bitwise_and(rgb_image3, rgb_image3, mask=mask)\n",
    "\n",
    "gs = plt.GridSpec(2, 2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(rgb_image3)\n",
    "plt.title('Исходное изображение')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Маска')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(gs[2])\n",
    "plt.hist(h.reshape(-1), np.max(h), [np.min(h), np.max(h)])\n",
    "plt.vlines(low_h, 0, 5000, 'r'), plt.vlines(high_h, 0, 5000, 'r')\n",
    "plt.title('Гистограмма h слоя')\n",
    "plt.subplot(gs[3])\n",
    "plt.imshow(result)\n",
    "plt.title('Изображение с пикселями выделенного цвета')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e68c7",
   "metadata": {},
   "source": [
    "# 2 Выделение границ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1792d",
   "metadata": {},
   "source": [
    "Традиционно, рассматриваются две модели краев: \"ступенька\" и \"излом\". Точкам контура типа \"ступенька\" соответствуют точки смены знака второй производной, а точкам контура типа \"излом\" - точки смены знака первой производной изображения. Здесь делается допущение, что изображение это непрерывная и два раза дифференцируемая функция f(x,y). В действительности это допущение будет корректным, если перед взятием производной - изображение сгладить (отфильтровать) тем или иным способом.\n",
    "\n",
    "Из методов обнаружения края при помощи различных дифференциальных операторов наиболее известны операторы Робертса (Roberts), Собеля (Sobel), Канни (Canny), оператор Лапласа и Лапласиан Гаусcиана (LoG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158498b2",
   "metadata": {},
   "source": [
    "## 2.1 Операторы вычисления векторов градиентов\n",
    "\n",
    "Операторы Робертса и Собеля реализуются путем свертки изображения со следующими ядрами:\n",
    "\n",
    "- Оператор Робертса:\n",
    "$\n",
    "M_1 =\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & -1\n",
    " \\end{pmatrix}\n",
    ";\n",
    "M_2 =\n",
    " \\begin{pmatrix}\n",
    "  0 & -1\\\\\n",
    "  1 & 0\n",
    " \\end{pmatrix}\n",
    ".\n",
    "$\n",
    "\n",
    "- Оператор Собеля:\n",
    "$\n",
    "M_x =\n",
    "\\begin{pmatrix}\n",
    " 1 & 0 & -1 \\\\\n",
    " 2 & 0 & -2 \\\\\n",
    " 1 & 0 & -1\n",
    " \\end{pmatrix}\n",
    ";\n",
    "M_y =\n",
    "\\begin{pmatrix}\n",
    " 1  & 2  & 1 \\\\\n",
    " 0  & 0  & 0 \\\\\n",
    " -1 & -2 & -1\n",
    " \\end{pmatrix}\n",
    ".\n",
    "$\n",
    "\n",
    "Исследования показали, что оператор Робертса не является в достаточной мере помехозащищенным. Оператор Собеля обеспечивает вполне удовлетворительные результаты при обработке реальных изображений.\n",
    "В каждой точке изображения приближённое значение величины градиента можно вычислить путём использования полученных приближенных значений производных:\n",
    "$ G = \\sqrt{G^2_x + G^2_y} $,\n",
    "где $ G_x = M_x * Image $, а  $ G_y = M_y * Image $\n",
    "Символ $*$ здесь - это операция свертки.\n",
    "Используя эту информацию, мы можем также вычислить направление градиента:\n",
    "${\\displaystyle \\mathbf {\\Theta } =\\operatorname {arctan} \\left({\\mathbf {G} _{y} \\over \\mathbf {G} _{x}}\\right)} $,\n",
    "где, к примеру, угол Θ равен нулю для вертикальной границы, у которой тёмная сторона слева.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv.imread('../images/lenna.png')\n",
    "gray_image1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY)\n",
    "rgb_image1 = cv.cvtColor(image1, cv.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = np.asarray([[1, 0], [0, -1]])\n",
    "M2 = np.asarray([[0, -1], [1, 0]])\n",
    "Mx = np.asarray([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "My = np.asarray([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "\n",
    "kernels = [M1, M2, Mx, My]\n",
    "\n",
    "gs = plt.GridSpec(3, 3)\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "titles = ['Оператор Робертcа M1', 'Оператор Робертcа M2', 'Оператор Собеля Mх',\n",
    "          'Оператор Собеля My', 'Величина градиента G', 'Направление градиента Θ']\n",
    "numberOfImage = 6\n",
    "shape = gray_image1.shape\n",
    "filtered_images = np.empty(shape[0] * shape[1] * numberOfImage).reshape(shape[0], shape[1], numberOfImage)\n",
    "\n",
    "for i in np.arange(numberOfImage - 2):\n",
    "    filtered_images[..., i] = cv.filter2D(gray_image1, -1, kernels[i])\n",
    "\n",
    "filtered_images[..., 4] = np.sqrt(filtered_images[..., 2] ** 2 + filtered_images[..., 3] ** 2)\n",
    "filtered_images[..., 5] = np.arctan2(filtered_images[..., 3], filtered_images[..., 2])\n",
    "\n",
    "for i in np.arange(numberOfImage):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title(titles[i])\n",
    "    plt.imshow(filtered_images[..., i], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5679bf",
   "metadata": {},
   "source": [
    "## 2.2 Операторы Лапласа и Лапласиан Гауссиана\n",
    "Рассмотрим операторы выделения краев на, основанных на вычислении производных.\n",
    "\n",
    "Простейшим оператором такого рода является оператор Лапласа.\n",
    "\n",
    "**оператор Лапласа** (Лапласиан) - это дифференциальный оператор, который является обобщением на функции нескольких переменных второй производной функции одной переменной. Обозначается символом $\\Delta$. Оператор Лапласа примененный к некоторой функции F ставит в соответствие функцию:\n",
    "$\\Delta F = \\frac{\\partial^2 F}{\\partial x^2_1} + \\frac{\\partial^2 F}{\\partial x^2_2} + \\dots + \\frac{\\partial^2 F}{\\partial x^2_n}$ для n-мерного пространства. В случае пространственной обработки изображений чаще всего $n=2$.\n",
    "\n",
    "Ядро лапласиана размером 3x3 выглядит так:\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  -1 & -1 & -1 \\\\\n",
    "  -1 & 8 & -1 \\\\\n",
    "  -1 & -1 & -1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "Эта маска позволяет в равной степени учитывать возможные перепады яркости во всех направлениях.\n",
    "\n",
    "**Лапласиан Гауссиана (LoG)** - это оператор выделения краев ступенчатого типа основанный на использовании оператора Лапласа, примененного после сглаживания изображения гауссовским линейным фильтром или непосредственно осуществляется свертка с маской $\\Delta G(\\sigma,x,y)$. Этот фильтр также известен как *разность гауссовских распределений* (difference of Gaussians ,DoG), т.к. форма маски $\\Delta G(\\sigma,x,y)$ хорошо аппроксимируется разностью гауссовских масок $G(\\sigma_1) - G(\\sigma_2)$ с соотношением $\\frac{\\sigma_1}{\\sigma_2} = 1.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "outImageDepth = cv.CV_16S  # глубина результирующего изображения. Т.к. входное изображение 8-битное, чтобы избежать переполнения сделаем выходное изображение 16-битное.\n",
    "\n",
    "gaussian33 = cv.GaussianBlur(gray_image1, (3, 3), 0)\n",
    "gaussian55 = cv.GaussianBlur(gray_image1, (5, 5), 0)\n",
    "\n",
    "# применяем оператор Лапласа к тестовому изображению\n",
    "laplace = cv.Laplacian(gray_image1, outImageDepth, ksize=3)\n",
    "# laplace = cv.convertScaleAbs(laplace)\n",
    "# вычисляем разность гауссовских разпределений\n",
    "dog = gaussian33 - gaussian55\n",
    "# dog = cv.convertScaleAbs(dog)\n",
    "# вычисляем Лапласиан Гауссиана\n",
    "log = cv.Laplacian(gaussian33, outImageDepth, ksize=3)\n",
    "log = cv.convertScaleAbs(log)\n",
    "\n",
    "# вывод\n",
    "plt.figure(figsize=(15, 8))\n",
    "gs = plt.GridSpec(1, 3)\n",
    "\n",
    "titles = ['Оператор Лапласа', 'Лапласиан Гауссиана', 'Разность гауссовских разпределений']\n",
    "outImages = [laplace, log, dog]\n",
    "\n",
    "for i in np.arange(len(outImages)):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title(titles[i])\n",
    "    plt.imshow(outImages[i], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813d6ad",
   "metadata": {},
   "source": [
    "## 2.3 Оператор Канни\n",
    "Оператор Канни использует многоступенчатый алгоритм для обнаружения границ в изображениях.\n",
    "Алгоритм Канни состоит из следующих шагов:\n",
    "- сглаживание изображения;\n",
    "- вычисление градиента. Используются четыре фильтра для обнаружения горизонтальных, вертикальных и диагональных ребер в размытом изображении;\n",
    "- подавление немаксимумов. Только локальные максимумы отмечаются как границы;\n",
    "- двойная пороговая фильтрация. Потенциальные границы определяются порогами;\n",
    "- трассировка области неоднозначности. Итоговые границы определяются путём подавления всех краёв, не связанных с определенными (сильными) границами.\n",
    "\n",
    "К счастью, нам не нужно реализовывать алгоритм Канни, т.к. его реализация есть в OpenCV.\n",
    "Алгоритм Канни реализован в OpenCV в функции Canny. Для его настройки его работы нужно передать функции два порога.\n",
    "Подробнее про алгоритм Канни можно почитать в [документации](https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64370c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 100\n",
    "threshold2 = 200\n",
    "edges = cv.Canny(gray_image1, threshold1, threshold2)\n",
    "\n",
    "# вывод\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121), plt.imshow(gray_image1, cmap='gray')\n",
    "plt.title('Исходное изображение'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122), plt.imshow(edges, cmap='gray')\n",
    "plt.title('Изображение краев, \\n вычисленных алгоритмом Канни'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de107f",
   "metadata": {},
   "source": [
    "# 3 Методы анализа изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb647ad",
   "metadata": {},
   "source": [
    "## 3.1 Математическая морфология Серра\n",
    "\n",
    "Математическая морфология Серра позволяет осуществлять обработку изображений с учетом формы и размера имеющихся на изображении областей. Морфологические операторы Серра позволяют: выделять или удалять на изображениях мелко- и среднеразмерные объекты заданной формы и размера, а также фильтровать (сглаживать) форму крупноразмерных объектов.\n",
    "\n",
    "Базовыми операциями математической морфологии Серра являются: дилатация (расширение) и эрозия (сжатие) изображения X структурирующим элементом B. На этих базовых операциях основаны также операции открытия и закрытия.\n",
    "\n",
    "Рассмотрим геометрический смысл операторов математической морфологии на примере обработки искусственного изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread('../images/A_letter.png')\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "noise_image = util.add_salt_and_peper_noise(image, 0.02)\n",
    "plt.imshow(image, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "dilation = cv.dilate(noise_image, kernel, iterations=1)\n",
    "erosion = cv.erode(noise_image, kernel, iterations=1)\n",
    "opening = cv.morphologyEx(noise_image, cv.MORPH_OPEN, kernel)\n",
    "closing = cv.morphologyEx(noise_image, cv.MORPH_CLOSE, kernel)\n",
    "closeAndOpen = cv.morphologyEx(opening, cv.MORPH_CLOSE, kernel)\n",
    "\n",
    "# вывод\n",
    "plt.figure(figsize=(15, 8))\n",
    "gs = plt.GridSpec(2, 3)\n",
    "\n",
    "titles = ['Зашумленное изображение', 'Дилатация', 'Эрозия', 'Открытие', 'Закрытие',\n",
    "          'Последовательное закрытие и открытие']\n",
    "outImages = [noise_image, dilation, erosion, opening, closing, closeAndOpen]\n",
    "\n",
    "for i in np.arange(len(outImages)):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title(titles[i])\n",
    "    plt.imshow(outImages[i], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96431f56",
   "metadata": {},
   "source": [
    "## 3.2 Сопоставление шаблонов\n",
    "\n",
    "Сопоставление шаблонов — это метод поиска фрагмента изображения (шаблона) и его местоположения в более крупном изображении. Работа метода основана на перемещении окна (ROI) по входному изображению (как в свертке) и вычислении метрики близости фрагмента изображения, ограниченного окном с шаблоном. Результатом этой работы является новое изображение, где каждый пиксель имеет значение метрики близости фрагмента изображения в текущей окрестности с шаблоном. В OpenCV для этого используется функция cv.matchTemplate().\n",
    "Когда есть выходное изображение с мерой близости можно использовать функцию cv.minMaxLoc() чтобы найти координаты пикселя с максимальным (минимальным) значением меры близости.\n",
    "В OpenCV реализовано несколько методов [сравнения](https://docs.opencv.org/3.4/df/dfb/group__imgproc__object.html#ga3a7850640f1fe1f58fe91a2d7583695d). От этого зависит максимальное или минимальное значение необходимо искать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0271e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv.imread('../images/lenna.png')\n",
    "gray_image = cv.cvtColor(bgr_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "ROI = (220, 210, 135, 180)  #(x0, y0, height, width)\n",
    "template = gray_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "\n",
    "w, h = template.shape[::-1]\n",
    "# Список всех 6 методов для сравнения\n",
    "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',\n",
    "           'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']\n",
    "for meth in methods:\n",
    "    method = eval(meth)\n",
    "\n",
    "    res = cv.matchTemplate(gray_image, template, method)\n",
    "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
    "    # Для методов TM_SQDIFF и TM_SQDIFF_NORMED берется минимум, для остальных максумум res\n",
    "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    cv.rectangle(gray_image, top_left, bottom_right, 255, 2)\n",
    "    plt.subplot(121), plt.imshow(res, cmap='gray')\n",
    "    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122), plt.imshow(gray_image, cmap='gray')\n",
    "    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "    plt.suptitle(meth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64970f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_image = cv.imread('../images/pear.png')\n",
    "rgb_image = cv.cvtColor(bgr_image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404ebfb",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Локальные особенности. Особые точки.\n",
    "\n",
    "Локальные особенности изображения - это хорошо различимые фрагменты объектов на изображении. Как правило, речь идет о поиске точек, каждая из которых обладает характерной окрестностью, отличающую эту точку от остальных.\n",
    " В разной литературе точки с локальными особенностями называют особыми точками (feature points или просто features), локальными особыми точками (local feature points) или характеристическими точками (characteristic points).\n",
    "\n",
    "При поиске особых точек, к ним предъявляются следующие требования:\n",
    "- повторимость, т.е. находились всегда в том же месте несмотря на изменения масшаба, положения, ракурса и освещения;\n",
    "- локальность. Особые точки должны определяться маленькой окрестностью, чтобы работа с ней не была чувствительна к перекрытиям;\n",
    "- значимость. Каждая точька должна иметь уникальное описание;\n",
    "- компактность и эффективность. Количество особых точек, должно быть значительно меньше пикселей изображения.\n",
    "\n",
    "Особые точки используются в задачах поиска и выделения объектов на изображениях, распознавания объектов на изображениях, для поиска изображений в базе данных или в файловой системе, сопоставления изображений, например, для построения карты местности.\n",
    "\n",
    "Алгоритмы поиска особых точек называются детекторами. Рассмотрим некоторые из них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e1757",
   "metadata": {},
   "source": [
    "## 4.1 Детектор Харриса\n",
    "\n",
    "Детектор Харриса - один из первых и самых популярных детекторов особых точек. Ищет такие точки, окрестность которых меняется при любом сдвиге. При создании этого детектора авторами (Крисом Харрисом и Майком Стивенсом) было предложено в качестве особых точек искать углы. и  Такими точками в Харрис и  предлагаются искать углы.\n",
    "\n",
    "Подробное описание алгоритма и пример его работы можно посмотреть на сайте [OpenCV](https://docs.opencv.org/4.5.5/dc/d0d/tutorial_py_features_harris.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b72997",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "\n",
    "cropped_image = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(cropped_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "gray_image = np.float32(gray_image)\n",
    "dst = cv.cornerHarris(gray_image, 2, 3, 0.04)\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "cropped_image[dst > 0.01 * dst.max()] = [255, 0, 0]\n",
    "plt.imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa210d",
   "metadata": {},
   "source": [
    "Если стоит задача поиска углов с субпиксельной точностью, можно использовать функцию cv.cornerSubPix(), которая дополнительно уточняет обнаруженные углы с точностью до субпикселя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "image = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# find Harris corners\n",
    "gray_image = np.float32(gray_image)\n",
    "dst = cv.cornerHarris(gray_image, 2, 3, 0.04)\n",
    "dst = cv.dilate(dst, None)\n",
    "ret, dst = cv.threshold(dst, 0.01 * dst.max(), 255, 0)\n",
    "dst = np.uint8(dst)\n",
    "# find centroids\n",
    "ret, labels, stats, centroids = cv.connectedComponentsWithStats(dst)\n",
    "# define the criteria to stop and refine the corners\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv.cornerSubPix(gray_image, np.float32(centroids), (5, 5), (-1, -1), criteria)\n",
    "# Now draw them\n",
    "res = np.hstack((centroids, corners))\n",
    "res = np.int0(res)\n",
    "image[res[:, 1], res[:, 0]] = [0, 0, 255]\n",
    "image[res[:, 3], res[:, 2]] = [0, 255, 0]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4244e",
   "metadata": {},
   "source": [
    "Чтобы увидеть результат, нужно приблизить получившееся изображение. Должно получиться как на рисунке:\n",
    "<img height=\"100\" src=\"../content/subpixel_corner_detection.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249687c",
   "metadata": {},
   "source": [
    "## 4.2 Детектор Shi-Tomasi\n",
    "Детектор Shi-Tomasi является усовершенствованным вариантом детектора Харриса.\n",
    "Для нахождения особых точек методом Shi-Tomasi в OpenCV есть функция goodFeaturesToTrack(). Она находит N сильнейших углов на изображении. В качестве параметров функции передается: желаемое число углов, которое нужно найти; число от 0 до 1, определяющее уровень \"качества\" углов, ниже которого найденые углы отсекаются; минимальное евклидово расстояние между найденными углами.\n",
    "Подробнее про функцию [goodFeaturesToTrack()](https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "image = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv.goodFeaturesToTrack(gray_image, 54, 0.01, 10)\n",
    "corners = np.int0(corners)\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv.circle(image, (x, y), 3, 255, -1)\n",
    "\n",
    "plt.imshow(image), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba87b5b",
   "metadata": {},
   "source": [
    "##  4.3 SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "SIFT -- это детектор инвариантный (нечувствительный) к масштабированию. Как известно, рассмотренные выше детекторы особых точек инвариантны к повороту изображения благодаря тому, что они обнаруживают углы. Но углы в одном масштабе могут не являться углами в другом масштабе.\n",
    "<img height=\"100\" src=\"../content/corners_example.png\"/>\n",
    "В отличии от детектора Харриса, SIFT является инвариантным к масштабированию. А в качестве особых точек ищет не углы, а иные точки изображения инвариантные к повороту, смещению, масштабированию, изменению освещенности и частично к аффинным преобразованиям.\n",
    "Подробнее в статье на сайте [OpenCV](https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html).\n",
    "\n",
    "Рассмотрим применение SIFT с использованием OpenCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b493de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "image = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv.SIFT_create()\n",
    "kp = sift.detect(gray_image, None)\n",
    "image = cv.drawKeypoints(gray_image, kp, image, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb44e2a",
   "metadata": {},
   "source": [
    "## 4.4 FAST\n",
    "Рассмотренные выше алгоритмы поиска особых точек на изображениях достаточно эффективны, но вычислительно сложные. Они не позволяют использовать их в приложениях реального времени на устройствах с ограниченными вычислительными ресурсами.\n",
    "\n",
    "Для решения этой проблемы был предложен алгоритм [FAST](https://docs.opencv.org/3.4/df/d0c/tutorial_py_fast.html).\n",
    "\n",
    "Применение детектора FAST с использованием OpenCV представлен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "img = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate FAST object with default values\n",
    "fast = cv.FastFeatureDetector_create()\n",
    "# find and draw the keypoints\n",
    "kp = fast.detect(img,None)\n",
    "img2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "# Print all default params\n",
    "print( \"Threshold: {}\".format(fast.getThreshold()) )\n",
    "print( \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression()) )\n",
    "print( \"neighborhood: {}\".format(fast.getType()) )\n",
    "print( \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp)) )\n",
    "cv.imwrite('fast_true.png', img2)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb5a63",
   "metadata": {},
   "source": [
    "## 4.5 ORB (Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "[ORB](https://docs.opencv.org/4.x/d1/d89/tutorial_py_orb.html) - это быстрый и надежный детектор особых точек, представленный в 2011 году. Он основан на детекторе FAST и модифицированном детектора BRIEF. Целью его создания было предоставление бесплатного детектора для замены запатентованного детектора SIFT, использование которого в коммерческих целях требовало лицензионных отчислений автору."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963cf307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rgb_image = cv.imread('../images/calibration/left01.jpg')\n",
    "ROI = (235, 75, 290, 200)  #(x0, y0, height, width)\n",
    "img = rgb_image[ROI[1]:ROI[1] + ROI[3], ROI[0]:ROI[0] + ROI[2]]\n",
    "gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv.ORB_create()\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(img,None)\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(img, kp)\n",
    "# draw only keypoints location,not size and orientation\n",
    "img2 = cv.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\n",
    "plt.imshow(img2), plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
